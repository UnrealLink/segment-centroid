
\input{common}

\title{Learning Hierarchical Control Policies from Demonstrations}
\date{}

\begin{document}

\maketitle

\section{What is a hierarchical control policy}

In a hierarchical control policy, the lowest level is a generalization of a standard control policy $\pi(a|s)$: it also takes in a signal $h$ generated by the higher level, $\pi_{\text{low}}(a|s,h)$.
The hint signal helps the low level planning by saving effort in processing state information, acting both like a short-term memory and like a short-term subgoal.
The signal changes slowly, and represents reuse of processing efforts between adjacent time steps.

The high level doesn't need to output a control signal $a$, only a meta-control signal $h$, according to a meta-policy $\pi_{\text{high}}(h|s)$.
It can plan on a longer timescale, and reuse the low-level planning efforts between distant time steps.

In our formulation, the low level decides when to terminate its control and return it to the high level, like a return from a function call.
This is in contrast to the DeepMind paper, where the low level terminates every 2 steps.

\section{Why use a hierarchical control policy}

\subsection{Policy complexity reduction}

The reinforcement learning problem is hard, partly due to the complexity of the solution space - all (stochastic) mappings of states into actions.
As we move toward harder problems, with richer state and action spaces, this problem very quickly becomes intractable.

The only way to scale up without an inhibiting amount of samples and computation, is to have an inductive bias - a prior assumption about the structure of the problem.
The hierarchical structure is very appealing, also for other reasons, but mostly because it can greatly reduce the complexity of the policy.
In principle, it can allow scaling the policy up to arbitrary system complexity and time horizons, and down to arbitrary spacial and temporal precision.

\subsection{Sensor/actuator complexity reduction}

One aspect of the reduction in complexity of the control function is the reduction in the dimensionality of its input and its output.
This has the benefit of simplifying sensors and actuators, which is important both computationally and for the practical engineering aspect.

In the DeepMind paper the low level achieves a hard-coded reduction in input complexity, by only taking part of the input observation.
We're taking a more principled approach, by learning an emergent input/output complexity reduction as part of our learning algorithm.

\subsection{Specialization and reusability}

As mentioned above, the low level specializes in short-term planning, while the high level specializes in long-term planning.
This is also reflected in how each level reuses the other: the low level reuses the high-level signal $h$ in multiple adjacent time steps, creating a short-term correlation that allows it to focus on a subgoal.
The high level, on the other hand, reuses the low-level primitives in multiple distant time steps, creating a long-term correlation between repeatable behaviors in similar states.

In the DeepMind paper, the high-level \emph{modulation} signal $h$ is continuous, and the low-level policy parameters are shared for all values of $h$.
We have a discrete \emph{switching} signal, that indexes a different control primitive for each $h$, like a named subroutine.
This allows these policies to be archived and shared discretely between various systems.

All but the highest-level primitives can be reused for new unseen tasks.
The highest level only needs to decide how to best use these precomputed options to perform the new task.

All but the lowest-level primitives can be reused for new unseen hardware.
The lowest level only needs to decide how to implement these subgoals on the new hardware.

\section{How to interpret a hierarchical control policy}

\subsection{As a structure / constraint on policy space}

A hierarchical control policy has a particular structure, that reduces the effective volume of the solution space.
However, it is not a special case of a flat control policy, since the high level can create temporal correlations.

For example, consider the task of repeatedly picking an object at the right end of a corridor and dropping it at the left end.
With flat control, the robot needs at each time step to examine the state and figure out what it needs to do: move right, pick up, move left or drop.
With hierarchical control, the high level switches between these subtasks, and the low level only needs to perform a primitive or detect its success.

The constraint on the low level comes from the simplified parameterization of the low-level primitives.
The constraint on the high level comes from the limited number of low-level primitives.

\subsection{As a modular control system}

The division of planning effort into hierarchical levels is a form of modularization.
This allows different parts of the computation to be distributed and run at different times and locations.
The only requirement is a slowly changing interface between the components.

\subsection{As a structured memory state representation}

The persistent high-level signal can be viewed as a memory state.
This memory state has a structure which is distributed between the levels, with higher-level features more persistent than lower-level ones.

Memory-based policies are not necessary for optimal control in fully-observable environments.
However, they become useful in the following cases:

\begin{itemize}
	\item In partially-observable environments, where past observations are useful in inferring the hidden state;
	\item To model controllers that have latent variables, such as human demonstrations;
	\item When memory is cheaper than perception, e.g. when persistent features, that can be kept constant in memory with very little processing, are useful in reducing the complexity of processing each new observation.
\end{itemize}

The space of general memory-based controllers is huge and contains many local optima, however the constraints mentioned above allow optimization over a useful subset of this space.

\subsection{As a stack of function calls}

The flow of control in a hierarchical policy follows the same pattern as in digital computer programs.
It starts at the highest level and goes down the hierarchy.
When each level finishes executing its policy, it terminates and returns control to the level above it.
The call stack represents the structured memory of the controller, with each frame a control signal to the level below.

\subsection{As a renormalization operator}

Humans can remember arbitrarily far histories and plan to arbitrarily long horizons.
This implies that their highest levels are a fixed point of the "next higher level" operator.
In statistical physics, this kind of self-similarity is captured by the notion of renormalization.
The properties of the hierarchy-construction operator can then be studied by expansion in the neighborhood of the renormalization operator's fixed point.

\section{How to learn a hierarchical control policy}

In the following we present algorithms for learning the low-level primitives from demonstrations, by using a naive model for the high level.
A similar algorithm can be used to learn while planning, rather than from demonstrations.

Each such algorithm can be viewed as:

\begin{itemize}
	\item A clustering algorithm, with the learned primitives as the centroids and the cluster assignment as the high-level control.
		Unsupervised algorithms for reinforcement learning are gaining a lot of attention in recent months.
	\item A maximum-likelihood optimization of a generative model.
		This has connections to variational inference, which has been of major interest in the past couple of years.
	\item Optimal control under the constraints of a communication model.
		This connects directly to a growing community interested in information-theoretical aspects of dynamical systems.
\end{itemize}

\subsection{Clustering subtask demonstrations}

Parameters:
\begin{itemize}
	\item $P\in\Delta([k])$
	\item $\forall h\in[k]$, $\theta_h\in\Theta$, inducing $\pi_h:\C S\to\Delta(\C A)$
\end{itemize}
Generative model:
\begin{itemize}
	\item For $1\le i\le m$, independently
	\begin{itemize}
		\item Draw $h_i\sim P$ (latent)
		\item Draw $\rho_i\sim\p_{h_i}$ (observed)
	\end{itemize}
\end{itemize}
Log likelihood of observed given latent:
\eq{
\C L(\rho|h)=\sum_i\log\p_{h_i}(\rho_i)=\sum_i\sum_{t=0}^{T_i-1}\log\pi_{h_i}(a_{i,t}|s_{i,t})+\const
}
Update iteration:
\eq{
q_i(h)&=\frac{P(h)\p_{\pi_h}(\rho_i)}{\sum_{h'}P(h')\p_{\pi_{h'}}(\rho_i)}=\frac{P(h)\prod_{t=0}^{T_i-1}\pi_h(a_{i,t}|s_{i,t})}{\sum_{h'}P(h')\prod_{t=0}^{T_i-1}\pi_{h'}(a_{i,t}|s_{i,t})}\\
P(h)&=\frac1m\sum_iq_i(h)\\
\delta\theta_h&=\alpha\frac1m\sum_iq_i(h)\sum_{t=0}^{T_i-1}\dif_{\theta_h}\log\pi_h(a_{i,t}|s_{i,t})
}

\subsection{Segmenting and clustering task demonstrations}

Same parameters.\\
Same generative model, except output first $T$ steps of $(\rho_i)_i$ concatenated (latent segmentation).\\
Same log likelihood of observed given latent (since segmentation is revealed).\\
Update iteration (with $\rho_{\le t}=(s_0,a_0,\ldots,s_t)$, $\rho_{>t}=(a_t,s_{t+1},a_{t+1},\ldots,s_T)$, $h_t$ the hint for $a_t$, and $\bot$ the termination action):
\eq{
q_{t,h}&\mathrel{\propto}\p(\rho,h_t=h)=\p(\rho_{\le t},h_t=h)\p(\rho_{>t}|s_t,h_t=h)\\
b_{t,h}&\mathrel{\propto}\p(\rho,h_t=h,\bot\text{ at }s_{t+1})=\sum_{h'}\p(\rho,h_t=h,\bot\text{ at }s_{t+1},h_{t+1}=h')\\
&=\sum_{h'}\p(\rho_{\le t},h_t=h)\pi_h(a_t|s_t)p(s_{t+1}|s_t,a_t)\psi_h(s_{t+1})P(h'|h)\p(\rho_{>t+1}|s_{t+1},h_{t+1}=h')\\
\p(\rho_{\le t+1},h_{t+1}=h')&=\sum_h\p(\rho_{\le t},h_t=h)\pi_h(a_t|s_t)p(s_{t+1}|s_t,a_t)(\psi_h(s_{t+1})P(h'|h)+(1-\psi_h(s_{t+1}))\delta_{h,h'})\\
\p(\rho_{>t}|s_t,h_t=h)&=\pi_h(a_t|s_t)p(s_{t+1}|s_t,a_t)\sum_{h'}(\psi_h(s_{t+1})P(h'|h)+(1-\psi_h(s_{t+1}))\delta_{h,h'})\p(\rho_{>t+1}|s_{t+1},h_t=h')\\
\delta\theta_h&=\alpha\sum_{t=0}^{T-1}(q_{t,h}\log\pi_h(a_t|s_t)+b_{t,h}\log\pi_h(\bot|s_t))
}
$p(s_{t+1}|s_t,a_t)$ can be dropped due to the normalization.

\end{document}